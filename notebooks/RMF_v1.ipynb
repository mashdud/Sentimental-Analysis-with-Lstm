{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f1a0328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "params = {'legend.fontsize': 'medium',\n",
    "          'figure.figsize': (10, 5),\n",
    "         'axes.labelsize': 'medium',\n",
    "         'axes.titlesize':'large',\n",
    "         'xtick.labelsize':'medium',\n",
    "         'ytick.labelsize':'medium'}\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73abf1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/drugsComTrain_raw.tsv\", delimiter='\\t')\n",
    "train = train.rename(columns = {\"Unnamed: 0\": \"uniqueID\"})\n",
    "train['date'] = pd.to_datetime(train['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0424dbd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniqueID</th>\n",
       "      <th>drugName</th>\n",
       "      <th>condition</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>usefulCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>206461</td>\n",
       "      <td>Valsartan</td>\n",
       "      <td>Left Ventricular Dysfunction</td>\n",
       "      <td>\"It has no side effect, I take it in combinati...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2012-05-20</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95260</td>\n",
       "      <td>Guanfacine</td>\n",
       "      <td>ADHD</td>\n",
       "      <td>\"My son is halfway through his fourth week of ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2010-04-27</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92703</td>\n",
       "      <td>Lybrel</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>\"I used to take another oral contraceptive, wh...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2009-12-14</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>138000</td>\n",
       "      <td>Ortho Evra</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>\"This is my first time using any form of birth...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2015-11-03</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35696</td>\n",
       "      <td>Buprenorphine / naloxone</td>\n",
       "      <td>Opiate Dependence</td>\n",
       "      <td>\"Suboxone has completely turned my life around...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2016-11-27</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uniqueID                  drugName                     condition  \\\n",
       "0    206461                 Valsartan  Left Ventricular Dysfunction   \n",
       "1     95260                Guanfacine                          ADHD   \n",
       "2     92703                    Lybrel                 Birth Control   \n",
       "3    138000                Ortho Evra                 Birth Control   \n",
       "4     35696  Buprenorphine / naloxone             Opiate Dependence   \n",
       "\n",
       "                                              review  rating       date  \\\n",
       "0  \"It has no side effect, I take it in combinati...     9.0 2012-05-20   \n",
       "1  \"My son is halfway through his fourth week of ...     8.0 2010-04-27   \n",
       "2  \"I used to take another oral contraceptive, wh...     5.0 2009-12-14   \n",
       "3  \"This is my first time using any form of birth...     8.0 2015-11-03   \n",
       "4  \"Suboxone has completely turned my life around...     9.0 2016-11-27   \n",
       "\n",
       "   usefulCount  \n",
       "0           27  \n",
       "1          192  \n",
       "2           17  \n",
       "3           10  \n",
       "4           37  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7c09aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31611871268529484"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## baseline classification model\n",
    "from sklearn.dummy import DummyClassifier\n",
    "X = train.iloc[:, 0:4]\n",
    "y = train['rating']\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(X,y)\n",
    "dummy_clf.predict(X)\n",
    "dummy_clf.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0da97f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline MSE:  10.70807206449809\n",
      "Baseline r-squared:  0.0\n"
     ]
    }
   ],
   "source": [
    "## baseline regression model\n",
    "from sklearn.dummy import DummyRegressor\n",
    "dummy_reg = DummyRegressor(strategy=\"mean\")\n",
    "dummy_reg.fit(X,y)\n",
    "y_pred = dummy_reg.predict(X)\n",
    "print(\"Baseline MSE: \", mean_squared_error(y, y_pred))\n",
    "print(\"Baseline r-squared: \", dummy_reg.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9df85de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import requests, json, os, sys, time, re\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "# import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed97aee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = {\n",
    "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is / how does\",\n",
    "\"I'd\": \"I had / I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I shall / I will\",\n",
    "\"I'll've\": \"I shall have / I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Text Preprocessing '''\n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "    # Expand contractions\n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text)\n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    text = re.sub(r'\\d+','', text)\n",
    "    # remove stopwords\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "    # Tokenize each word\n",
    "    text =  nltk.WordPunctTokenizer().tokenize(text)\n",
    "    # Stemming\n",
    "#     lemm = nltk.stem.WordNetLemmatizer()\n",
    "#     text = list(map(lambda word:list(map(lemm.lemmatize, word)), text))\n",
    "    \n",
    "    stemmer_snowball = SnowballStemmer('english')\n",
    "    stem_words = []\n",
    "    for w in text:\n",
    "        x = stemmer_snowball.stem(w)\n",
    "        stem_words.append(x)\n",
    "    return stem_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47447d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"I&#039;m 30 years old.  I started having really bad skin about 2 years ago from maybe stress in my life.  But, I tried EVERYTHING to get rid of it and it just got worse.  I finally went to a dermatologist and she put me on spironolactone.  My skin started getting better in about two weeks. I LOVED it. I&#039;ve been taking it for about 6 months and I get compliments from Makeup artist and models (I&#039;m a hairstylist) about how good my skin is.  Another plus for me is my breasts went from a small B to a C! I decided to stop taking it for a week and my breasts went small again and my skin is getting a little oily. So I&#039;m going back on. I only take one a day.\"'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['review'][98]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3148beac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['review_cleaned'] = train['review'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ad2f39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "904c375d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_transform = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=[1,1], lowercase=False)\n",
    "tfidf_transform = TfidfTransformer(norm=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e81ed047",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[['review_cleaned','usefulCount']]\n",
    "y = train['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6f0f19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42c9608c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bow = bow_transform.fit_transform(X_train['review_cleaned'])\n",
    "X_tfidf = tfidf_transform.fit_transform(X_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78a79b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120972, 29610)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5eb280c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_features=2000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## modeling\n",
    "rmf = RandomForestRegressor(max_features = 2000)\n",
    "rmf.fit(X_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8858412",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bow_test = bow_transform.transform(X_test['review_cleaned'])\n",
    "X_tfidf_test = tfidf_transform.transform(X_bow_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "839336e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.278324741735584"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = rmf.predict(X_tfidf_test)\n",
    "mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca10071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c54814c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rmf.pkl']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(rmf, 'rmf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "febff775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5967521462247045"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089a1586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train classification model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
